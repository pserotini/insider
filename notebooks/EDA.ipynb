{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae57d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import MinMaxScaler, OrdinalEncoder\n",
    "# from pandas.testing import assert_frame_equal\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from src.preprocessing import TitanicPreprocessor\n",
    "\n",
    "import mlflow\n",
    "\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4836800d",
   "metadata": {},
   "source": [
    "## Overview\n",
    "### Este notebook contém as seguintes seções:\n",
    "- Seção 1: EDA\n",
    "- Seção 2: Feature Engineering\n",
    "- Seção 3: Modelagem\n",
    "- Seção 4: Preparação de pipeline de pre-processing para uso na api"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98dc3e4",
   "metadata": {},
   "source": [
    "### Seção 1 - EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9e04ac",
   "metadata": {},
   "source": [
    "Primeiramente, analisa-se o dataset que apresenta Ground Truth para entendermos a característica geral dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62e5c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_dataset = pd.read_csv('./data/train.csv')\n",
    "gt_dataset.columns = gt_dataset.columns.str.lower()\n",
    "gt_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386317ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = ProfileReport(gt_dataset,title='Titanic Dataset', explorative=True)\n",
    "profile.to_file('./profile.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6798b2e",
   "metadata": {},
   "source": [
    "Com o auxílio do relatório, podemos já fazer algumas observações:\n",
    "- 'cabin' tem um número excessivo de missing values, então será descartado\n",
    "- há significativas correlações de 'survived' com: 'fare', 'pclass' e 'sex'\n",
    "- várias colunas têm grande desbalanceamento, o que poderia ser tratado com, por exemplo, resampling numa análise mais aprofundada\n",
    "- 'ticket' não tem qualquer missing value e tem valores repetidos, podendo assim ser utilizado com um proxy para famílias e ter maior sucesso nessa \\\n",
    "função de proxy quando comparado a se extrair o sobrenome da coluna 'name', já que pode haver sobrenomes populares, comuns a pessoas desconhecidas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e900272",
   "metadata": {},
   "source": [
    "### Seção 2 - Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357a1cd6",
   "metadata": {},
   "source": [
    "Além de 'cabin', eliminamos aqui também 'passengerid' que é só um identificador (poderíamos fazer uma análise mais aprofundada para entender se a coluna relaciona de alguma forma com posição no barco, etc..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7986b51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_dataset.drop(columns=['cabin','passengerid'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55c6091",
   "metadata": {},
   "source": [
    "Dado o contexto do dado, sabemos que pode ser uma boa ideia tratar a idade por grupos, pois a evacuação priorizava crianças. Assim, faremos um encoding por grupo de idade, com uma resolução de 10 anos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25069923",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_bins = [age_bin for age_bin in range(0, int(gt_dataset['age'].max())+10, 10)]\n",
    "\n",
    "os.makedirs(\"pickle_files\", exist_ok=True)\n",
    "joblib.dump(age_bins, \"pickle_files/age_bins.pkl\")\n",
    "\n",
    "gt_dataset[\"age_group\"] = pd.cut(gt_dataset[\"age\"], bins=age_bins, right=False).cat.codes\n",
    "gt_dataset.drop(columns='age',inplace=True)\n",
    "gt_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6878f3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "004e08cc",
   "metadata": {},
   "source": [
    "Em relação ao ticket, vamos extrair o ticket count para atribuir a cada passageiro a informação de 'tamanho da família', já criar uma feature 'is_alone' para identificar se apenas aquele passageiro tem um determinado ticket e finalmente aplicar encoding para 'ticket'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d0bbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_dataset[\"ticket_count\"] = gt_dataset.groupby(\"ticket\")[\"ticket\"].transform(\"count\")\n",
    "gt_dataset[\"is_alone\"] = (gt_dataset[\"ticket_count\"] == 1)\n",
    "\n",
    "ticket_encoder = OrdinalEncoder()\n",
    "gt_dataset[\"ticket\"] = ticket_encoder.fit_transform(gt_dataset[[\"ticket\"]]).astype(int)\n",
    "\n",
    "joblib.dump(ticket_encoder, \"pickle_files/ticket_encoder.pkl\")\n",
    "\n",
    "# gt_dataset[\"ticket\"] = pd.factorize(gt_dataset[\"ticket\"])[0]\n",
    "\n",
    "gt_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a191039c",
   "metadata": {},
   "source": [
    "Finalizando o feature engineering para esta análise simplificada, vamos aplicar mais alguns scalings, encodings e remover colunas que não interessam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f93783",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_scale = [\"sibsp\", \"parch\", \"fare\", \"ticket_count\"]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "gt_dataset[cols_to_scale] = scaler.fit_transform(gt_dataset[cols_to_scale])\n",
    "\n",
    "joblib.dump(scaler, \"pickle_files/scaler.pkl\")\n",
    "\n",
    "gt_dataset = pd.get_dummies(gt_dataset, columns=[\"sex\", \"embarked\"], drop_first=True)\n",
    "gt_dataset.drop(columns='name',inplace=True)\n",
    "gt_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc33e57b",
   "metadata": {},
   "source": [
    "### Seção 3 - Modelagem\n",
    "Como se trata de um dataset Kaggle de competição, o test dataset não possui Ground Truth e não tem muita serventia para nós. Dessa forma, para tentar mitigar overfitting, vamos prosseguir com uma análise K-Fold no gt_dataset, para que os modelos sejam avaliados para diversos splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bc0b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = gt_dataset.drop(columns=[\"survived\"])\n",
    "y = gt_dataset[\"survived\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bd6f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"SVM\": SVC(kernel=\"rbf\", probability=True),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"Bagging\": BaggingClassifier(random_state=42),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c88e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9383c7fc",
   "metadata": {},
   "source": [
    "Agora instanciaremos o MLFlow para logar os modelos com suas métricas e metadados, e com uma automação simples salvaremos o que tiver melhor pontuação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555d87fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"titanic-models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099c0820",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = 0\n",
    "best_model = None\n",
    "best_model_name = \"\"\n",
    "\n",
    "\n",
    "for name, model in models.items():\n",
    "    with mlflow.start_run(run_name=name):\n",
    "        # Avaliação por f1-score e K-Fold\n",
    "        scores = cross_val_score(model, X, y, cv=cv, scoring=\"f1\")\n",
    "        mean_f1 = scores.mean()\n",
    "        std_f1 = scores.std()\n",
    "\n",
    "        # Treinamento\n",
    "        model.fit(X, y)\n",
    "\n",
    "        # Logging no MLFlow\n",
    "        mlflow.log_params(model.get_params())\n",
    "\n",
    "        mlflow.log_metric(\"f1_mean\", mean_f1)\n",
    "        mlflow.log_metric(\"f1_std\", std_f1)\n",
    "\n",
    "        mlflow.sklearn.log_model(\n",
    "            sk_model=model,\n",
    "            name=name,\n",
    "            registered_model_name=f\"{name.replace(' ', '_')}_Titanic\"\n",
    "        )\n",
    "\n",
    "        print(f\"{name} → f1-score médio: {mean_f1:.4f} (± {std_f1:.4f})\")\n",
    "        print(\"\")\n",
    "\n",
    "        if mean_f1 > best_score:\n",
    "            best_score = mean_f1\n",
    "            best_model = model\n",
    "            best_model_name = name        \n",
    "\n",
    "if best_model:\n",
    "    filename = f\"pickle_files/selected_model.pkl\"\n",
    "    joblib.dump(best_model, filename)\n",
    "    print(f\"\\nMelhor modelo salvo em: {filename} (f1-score = {best_score:.4f})\")            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0decc65d",
   "metadata": {},
   "source": [
    "### Seção 4: Pipeline de pré-processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710ee881",
   "metadata": {},
   "source": [
    "Aqui com o intuito de padronizar o pré-processamento, exportamos a pipeline de pré-processamento pra o pickle file `preprocessor.pkl` que replica todos os passos feitos neste notebook, a ser utilizada pela API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e0d3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_preproc = TitanicPreprocessor(scaler,ticket_encoder,age_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd45cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_dataset_pipeline = pd.read_csv('./data/train.csv')\n",
    "gt_dataset_pipeline.columns = gt_dataset_pipeline.columns.str.lower()\n",
    "gt_dataset_pipeline = pipeline_preproc.transform(gt_dataset_pipeline)\n",
    "gt_dataset_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff115f3",
   "metadata": {},
   "source": [
    "Confirmando que o dataset pré-processado utilizando pipeline é igual ao dataset pré-processado sem usar pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ebc8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if gt_dataset_pipeline.equals(gt_dataset.drop(columns='survived')):\n",
    "    print(\"## Pipeline de pré-processamento validada! ##\")\n",
    "    joblib.dump(pipeline_preproc, \"pickle_files/preprocessor.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
